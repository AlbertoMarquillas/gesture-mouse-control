# Gesture Mouse Control

![Language](https://img.shields.io/badge/language-Python-blue)
![Framework](https://img.shields.io/badge/framework-OpenCV-green)
![Library](https://img.shields.io/badge/library-MediaPipe-orange)
![License](https://img.shields.io/badge/license-MIT-green)
![Release](https://img.shields.io/github/v/release/AlbertoMarquillas/gesture-mouse-control)
![Conventional Commits](https://img.shields.io/badge/Conventional%20Commits-1.0.0-yellow)

---

## ğŸ“Œ Overview

The **Gesture Mouse Control** project demonstrates how to use **hand-tracking with OpenCV and MediaPipe** to emulate system actions using real-time hand gestures. In this implementation, the gestures are mapped to **volume control**, but the framework can be extended to emulate other mouse or keyboard actions.

The project showcases how computer vision and humanâ€“computer interaction can be combined to build natural and intuitive interfaces.

---

## ğŸ“‚ Repository Structure

```
<repo>/
â”œâ”€ src/                     # Source code
â”‚  â”œâ”€ main.py               # Entry point: runs the volume control app
â”‚  â””â”€ hand_tracking_module.py # Hand tracking module (mediapipe wrapper)
â”œâ”€ test/                    # Placeholder for future tests
â”œâ”€ docs/                    # Documentation and assets
â”‚  â””â”€ assets/               # Figures, screenshots, diagrams
â”œâ”€ notebooks/               # Optional experimentation notebooks
â”œâ”€ build/                   # Temporary outputs (ignored)
â”œâ”€ requirements.txt         # Dependencies
â””â”€ README.md                # Project documentation
```

---

## âš™ï¸ Installation

Ensure you have **Python 3.9+**. Install the dependencies listed in `requirements.txt`:

```powershell
git clone https://github.com/AlbertoMarquillas/gesture-mouse-control.git
cd gesture-mouse-control
pip install -r requirements.txt
```

Typical dependencies include:

* `opencv-python`
* `mediapipe`
* `numpy`

---

## ğŸš€ Usage

Run the application from the command line:

```powershell
python src/main.py
```

* The webcam feed will open.
* The **hand landmarks** are detected in real time.
* Gestures (e.g., distance between thumb and index fingers) are mapped to **system volume control**.

> âš ï¸ Note: the current implementation modifies system volume. To extend it for **mouse control** or **other actions**, adjust the mapping logic inside `main.py`.

---

## ğŸ” Features

* **Real-time hand tracking** using **MediaPipe**.
* **Gesture recognition** mapped to system actions.
* **Volume control demo** included by default.
* Modular code: `hand_tracking_module.py` encapsulates the MediaPipe logic.
* Easily extendable to support other gestures and actions.

---

## ğŸ“š What I Learned

Through this project I gained experience in:

* Using **OpenCV** for real-time video processing.
* Applying **MediaPipe** for robust hand-tracking.
* Mapping geometric relationships (distance between landmarks) to actionable commands.
* Designing intuitive humanâ€“computer interaction prototypes.
* Structuring Python projects for clarity and reusability.

---

## ğŸ“œ License

This project is released under the [MIT License](LICENSE).
